{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction to Image Processing: OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Why OpenCV\n",
    "\n",
    "- It is opensource\n",
    "- It is the standard de facto library for image processing and computer vision\n",
    "- It is integrated in other frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Let's Dive into OpenCV!\n",
    "\n",
    "A little reminder: this is a PRACTICAL course. Hence, if, needed, we will see the minimum theory required to use the methods and the algorithms that we are going to use and/or implement!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "As every Python module, we must import OpenCV before use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now, we can read an image placed in a folder. This can be done with the `imread` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('data/trex.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As it is possible to see, the read image is stored into the variable `img`. But what is the type of this variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### It is a Numpy array!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Since it is a numpy array, we can access to image information as we would access to a standard numpy matrix information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The width of the image is {img.shape[1]} pixels')\n",
    "print(f'The height of the image is {img.shape[0]} pixels')\n",
    "print(f'The number of channels of the image is {img.shape[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "There is no need to say that we can show the image on the screen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The first parameter of the `imshow` function is the name that will appear on the image window, while the second parameter is the numpy array containing the image that we want to show.\n",
    "\n",
    "The `waitKey` function, instead, waits for the number of milliseconds specified as argument, then, it closes the image window. If we want to show the image until we explicitly close the window, we just need to pass 0 as argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Lastly, we can change the extension of the image, by writing on disk the image that we have in memory.\n",
    "\n",
    "This is done with the `imwrite` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imwrite('data/out.jpg',img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Access to Image Elements\n",
    "\n",
    "We have seen that an OpenCV image is stored as a numpy array. Hence, we can access to its elements, namely the pixels, as we would do with a standard numpy array.\n",
    "\n",
    "Let's load again the T-Rex image and check that it has correctly loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('data/trex.png')\n",
    "cv2.imshow(\"Rex\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "To access to any pixel of the image, we just need its *x* and *y* coordinates. Concerning the channels, due to a historical reason OpenCV stores them in the BGR format. Keep this in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Images are just NumPy arrays. The top-left pixel can be\n",
    "# found at (0, 0)\n",
    "(b, g, r) = img[0, 0]\n",
    "print(\"Pixel at (0, 0) - Red: {}, Green: {}, Blue: {}\".format(r, g, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can change the value of a pixel by accesing to it and putting the new value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, let's change the value of the pixel at (0, 0) and\n",
    "# make it red\n",
    "img[0, 0] = (0, 0, 255)\n",
    "(b, g, r) = img[0, 0]\n",
    "print(\"Pixel at (0, 0) - Red: {}, Green: {}, Blue: {}\".format(r, g, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can also access to a whole portion of an image at once by using slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corner = img[0:100, 0:100]\n",
    "cv2.imshow(\"Corner\", corner)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "And we can modify such portion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's make the top-left corner of the image green\n",
    "img[0:100, 0:100] = (0, 255, 0)\n",
    "\n",
    "# Show our updated image\n",
    "cv2.imshow(\"Updated\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Drawing Stuff\n",
    "\n",
    "We have just seen how to draw on an image by setting a sub-matrix to a specific color. OpenCV provides several methods for drawing on images, namely, `line`, `rectangle`, and `circle`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Before starting to draw things, we should define our canvas. Off course we can load an image from the disk (we know how to do it!), but we can also create it from scratch (the best option!). How can we do that?\n",
    "\n",
    "### With Numpy arrays!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "canvas = np.zeros((300,300,3), dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Pay attention to the `dtype` of the canvas. Why it is `uint8`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Because each image channel has values in the range of [0,255], namely, a bit without sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now that the canvas has been initialized, we can draw something in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw a green line from the top-left corner of our canvas\n",
    "# to the bottom-right\n",
    "green = (0, 255, 0)\n",
    "cv2.line(canvas, (0, 0), (300, 300), green)\n",
    "cv2.imshow(\"Canvas\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook\n",
    "\n",
    "# Now, draw a 3 pixel thick red line from the top-right\n",
    "# corner to the bottom-left\n",
    "red = (0, 0, 255)\n",
    "cv2.line(canvas, (300, 0), (0, 300), red, 3)\n",
    "cv2.imshow(\"Canvas\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As it is possible to see, drawing a line is quite simple. However, there is another paramenter that we can consider while drawing a line: the tickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, draw a 3 pixel thick red line from the top-right\n",
    "# corner to the bottom-left\n",
    "red = (0, 0, 255)\n",
    "cv2.line(canvas, (300, 0), (0, 300), red, 3)\n",
    "cv2.imshow(\"Canvas\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook\n",
    "\n",
    "# Draw a green 50x50 pixel square, starting at 10x10 and\n",
    "# ending at 60x60\n",
    "cv2.rectangle(canvas, (10, 10), (60, 60), green)\n",
    "cv2.imshow(\"Canvas\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook\n",
    "\n",
    "# Draw another rectangle, this time we'll make it red and\n",
    "# 5 pixels thick\n",
    "cv2.rectangle(canvas, (50, 200), (200, 225), red, 5)\n",
    "cv2.imshow(\"Canvas\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "You may see that, together with the line, two rectangles have been drawn. The parameters of the rectangle function are: \n",
    "\n",
    "- the image in which you want to draw the rectangle (as for the line), the startin  \n",
    "- The starting point (x,y) of the rectangle (i.e., the top-left corner point)\n",
    "- The ending point (x',y') of the rectangle (i.e., the bottom-right corner point)\n",
    "- The color to use while drawing the rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It is possible to control also the tickness of the rectangle, such as for the lines. But what if we want to draw a filled rectangle?\n",
    "\n",
    "We just need to use a negative tickness value (usually -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's draw one last rectangle: blue and filled in\n",
    "blue = (255, 0, 0)\n",
    "cv2.rectangle(canvas, (200, 50), (225, 125), blue, -1)\n",
    "cv2.imshow(\"Canvas\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Drawing circles is just as simple as drawing rectangles, but the function arguments are a little different. Letâ€™s go ahead and get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset our canvas and draw a white circle at the center\n",
    "# of the canvas with increasing radii - from 25 pixels to\n",
    "# 150 pixels\n",
    "canvas = np.zeros((300, 300, 3), dtype = \"uint8\")\n",
    "(centerX, centerY) = (canvas.shape[1] // 2, canvas.shape[0] // 2)\n",
    "white = (255, 255, 255)\n",
    "\n",
    "for r in range(0, 175, 25):\n",
    "\tcv2.circle(canvas, (centerX, centerY), r, white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As it is possible to see, the parameters of the `circle` function are:\n",
    "\n",
    "- The image in which we want to draw the circle\n",
    "- The center of the circle in (x,y) coordinates\n",
    "- The radius of the circle, in pixels\n",
    "\n",
    "You can have some fun by using random values!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's go crazy and draw 25 random circles\n",
    "for i in range(0, 25):\n",
    "\t# randomly generate a radius size between 5 and 200,\n",
    "\t# generate a random color, and then pick a random\n",
    "\t# point on our canvas where the circle will be drawn\n",
    "\tradius = np.random.randint(5, high = 200)\n",
    "\tcolor = np.random.randint(0, high = 256, size = (3,)).tolist()\n",
    "\tpt = np.random.randint(0, high = 300, size = (2,))\n",
    "\n",
    "\t# draw our random circle\n",
    "\tcv2.circle(canvas, tuple(pt), radius, color, -1)\n",
    "\n",
    "# Show our masterpiece\n",
    "cv2.imshow(\"Canvas\", canvas)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Image Processing\n",
    "\n",
    "In the following slides, we will start to see some image processing basic techniques.\n",
    "\n",
    "The first thing that we are going to see is how to work with channels. First, load a sample image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('data/trex.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now let's get the channel of the image. In this case, the image is a simple RGB 3-channel image, so we can use the `split` function to store the 3 channels in 3 separate Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(r, g, b) = cv2.split(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "So, we can now plot, for example, the red channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv2.imshow(\"Red Channel\", r)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # needed only for jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Two questions should arise:\n",
    "\n",
    "- Why I do not see anything in red?\n",
    "- Since it is in greyscale, is this the true red channel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The answer for the first question is pretty simple. If we use only one channel, our image will be a one channel image, hence a greyscale one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The short answer for the second is: no, it is not the real red channel. As stated before, OpenCV store channels in BGR order, so that is the blue channel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Suppose that now that we have 3 separate channels, we want to merge them together to build an RGB image. We can simply use the `merge` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_copy = cv2.merge((r,g,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Remember that the `split` function is time consuming. To speed up the access to a specific channel, you can access to it by using the standard Numpy notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the blue channel of the image\n",
    "blue = img[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In this way, you can also set the pixels of a channel to a specific value. For example, we want to sett all the pixels of the blue channel to black:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img[:,:,0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Image Transformations\n",
    "The next thing that we are going to see is how to apply transformations to images. In detail, we will look at some examples the of scaling, translation, rotation, affine transformation, and perspective transform of images. These transformations can be applied by using the functions `cv2.warpAffine()` and `cv2.warpPerspective()`. The former requires a $2 \\times 3$ transformation matrix, while the latter requires a $3 \\times 3$ transformation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Image Scaling\n",
    "All the above mentioned transformations can be applied with the `cv2.warpAffine()` and `cv2.warpPerspective()`. However, for scaling and image, it is convenient to use the built-in OpenCV function `resize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resized_image = cv2.resize(img, (width * 2, height * 2), interpolation=cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In this way, we are resizing the image by explicitly setting the new dimensions. It is possible also to resize an image by defining the scaling factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resize the image by 50%\n",
    "resized_image = cv2.resize(img, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The `interpolation` parameter is used to choose how to create the missing points. The list of interpolation methods are reported in the OpenCV documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Image Translation\n",
    "\n",
    "To translate an object, i.e. an image, we have to fill our famous $2 \\times 3$ matrix providing the value for the translation in both $x$ and $y$ directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = np.float32([[1, 0, x], [0, 1, y]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "This gives the following M transformation matrix:\n",
    "\n",
    "$$M = \\begin{bmatrix}\n",
    "1 & 0 & t_x\\\\\n",
    "0 & 1 & t_y\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Once this matrix has been created, the `cv2.warpAffine()` function is called, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dst_image = cv2.warpAffine(img, M, (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The `cv2.warpAffine()` function transforms the source image using the M matrix provided. The third `(width, height)` argument establishes the size of the output image. For example, if we want to translate an image with $200$ pixels in the $x$ direction and $30$ pixels in the $y$ direction, we use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height, width = image.shape[:2]\n",
    "M = np.float32([[1, 0, 200], [0, 1, 30]])\n",
    "dst_image = cv2.warpAffine(image, M, (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Of course, the translation can also be negative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Image Rotation\n",
    "\n",
    "In order to rotate the image, we make use of the `cv.getRotationMatrix2D()` function to build the $2 x 3$ transformation matrix. This matrix rotates the image at the desired angle (in degrees), where positive values indicate a counterclockwise rotation. Both the center of rotation and the scale factor can also be adjusted. Usually, the roation matrix has the following form:\n",
    "\n",
    "$$\\begin{bmatrix} \\alpha & \\beta & (1- \\alpha ) \\cdot \\texttt{center.x} - \\beta \\cdot \\texttt{center.y} \\\\ - \\beta & \\alpha & \\beta \\cdot \\texttt{center.x} + (1- \\alpha ) \\cdot \\texttt{center.y} \\end{bmatrix}$$\n",
    "\n",
    "where $$\\begin{array}{l} \\alpha = \\texttt{scale} \\cdot \\cos \\texttt{angle} , \\\\ \\beta = \\texttt{scale} \\cdot \\sin \\texttt{angle} \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The following example builds the$ M$ transformation matrix to rotate 180 degrees with respect to the center of the image with a scale factor of 1 (without scaling). Afterwards, this M matrix is applied to the image, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "height, width = image.shape[:2]\n",
    "M = cv2.getRotationMatrix2D((width / 2.0, height / 2.0), 180, 1)\n",
    "dst_image = cv2.warpAffine(image, M, (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Affine Transformation\n",
    "\n",
    "In an affine transformation, we first make use of the `cv2.getAffineTransform()` function to build the 2 x 3 transformation matrix, which\n",
    "will be obtained from the input image and the coordinates of the transformed image. The latter can be also the new points defined by us!\n",
    "\n",
    "Once that the affine matrix has been computed, we can use it with the `cv2.warpAffine()` to apply the transformation to all the pixels forming the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pts_1 = np.float32([[135, 45], [385, 45], [135, 230]])\n",
    "pts_2 = np.float32([[135, 45], [385, 45], [150, 230]])\n",
    "M = cv2.getAffineTransform(pts_1, pts_2)\n",
    "dst_image = cv2.warpAffine(image_points, M, (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "> Remember that an affine transformation is a transformation where points, straight lines, and planes are preserved. Additionally, the parallel lines will remain parallel after this transformation. However, an affine transformation does not preserve both the distance and angles between\n",
    "points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Perspective Transformation\n",
    "\n",
    "In some cases, there is the need to correct the whole perspective of an image, e.g. when you create a panorama. As for the affine transformation, we first need to get the tranformation matrix, then we can apply the transformation to all the pixels of the image. Notice that, to apply the perspective transformation, we need 4 points instead of three. This is due to the Degrees of Freedom (DoF) that the matrix allows you to handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pts_1 = np.float32([[450, 65], [517, 65], [431, 164], [552, 164]])\n",
    "pts_2 = np.float32([[0, 0], [300, 0], [0, 300], [300, 300]])\n",
    "M = cv2.getPerspectiveTransform(pts_1, pts_2)\n",
    "dst_image = cv2.warpPerspective(image, M, (300, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### A Resume on Image Transformations\n",
    "\n",
    "These are all the affine transformations:\n",
    "\n",
    "<img src=\"imgs/1.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "while these are the differences between them and the projective transformation:\n",
    "\n",
    "<img src=\"imgs/2.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Image Filtering\n",
    " \n",
    " In this section, we are going to see how to apply filters on an image. These filters can enhance the contrast, can sharpen the image, can blur it, or all these stuff together. These filters are applied through an operation called convolution. \n",
    " \n",
    "### What is a Convolution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"imgs/3.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "A convolution operation consists in sliding a matrix of weights, namely, a *kernel*, over the input image in order to apply such weights to the image (or to extract features, as we will see later). In OpenCV, it is possible to apply a custom kernel to an image by using the function `cv2.filter2D`. In order to see how this function works, we should first build the kernel that we will use with the function. Let's create a $3 \\times 3$ kernel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_kernel = np.array([1,0,1], [1,0,1], [1,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Then, we can apply the just defined kernel in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernelized_image = cv2.filter2D(img, -1, my_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The argument of the function are pretty straightforward except for the second argument. In this case, -1 means that the output image will have the same depth of the input image. For a list of parameters, check the OpenCV documentation.\n",
    "\n",
    "Notice that some kernels are well known for performing certain operations, such as blurring or edge detection. Hence, instead of manually defining the kernel, OpenCV provides these kernels as ready-to-use functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "These functions are:\n",
    "\n",
    "- blur\n",
    "- boxfilter\n",
    "- GaussianBlur\n",
    "- medianBlur\n",
    "- bilateralFilter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Averaging Filters\n",
    "\n",
    "blur and boxfilter filters are used to average the pixels of an image. They simply take the average of the pixels under the kernel area and replace the central element with this average. Instead, in GaussianBlur the values of the kernel follow the Gaussian distribution. In fact, together with the kernel size, you can specifi the standard deviation on the $x$ or $y$ axis.\n",
    "\n",
    "The medianBlur is a type of blurring that allows to remove the salt and pepper noise from images, while the bilateralFilter allows to apply the blur but also to preserve the edges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sharpening Filters\n",
    "\n",
    "There are 2 ways for sharpen an image. The first consists in using an *unsharp mask*, i.e. a smoothed version of the image is subtracted from the original image. The second consists in defining a *sharpen* kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of unsharpen mask\n",
    "smoothed = cv2.GaussianBlur(img, (9, 9), 10)\n",
    "unsharped = cv2.addWeighted(img, 1.5, smoothed, -0.5, 0)\n",
    "\n",
    "# Example with sharpen kernel\n",
    "# Try different kernels for sharpening:\n",
    "kernel_sharpen = np.array([[0, -1, 0],\n",
    "                           [-1, 5, -1],\n",
    "                           [0, -1, 0]])\n",
    "sharpen_image = cv2.filter2D(img, -1, kernel_sharpen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Contour Detection\n",
    "\n",
    "There are some well known kernels that allows to extract only the edges from the image. The most famous and used are the *Sobel* and *Laplacian* kernels. The first computes the first order derivative, while the second adds together the second derivative computed on the *x* and *y* axis. Let's see them a little bit in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The Sobel operator is a kernel that compute (aproximatively) the first derivative of pixels intensity. But why the derivative?\n",
    "\n",
    "Let's look at the following images:\n",
    "\n",
    "<img src=\"imgs/4.jpg\" height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It is easy to see that in an edge, the pixel value changes noticeably, and as we know from the calculus course, the derivative is the best way to measure the change in a function. As it is possible to see in the following image, an edge is a *jump* in the pixel intensity:\n",
    "\n",
    "<img src=\"imgs/5_1.jpg\" height=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we take the first derivative, it is easier to see such jump as it would be a peak:\n",
    "\n",
    "<img src=\"imgs/5_2.jpg\" height=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The Sobel operator aims to compute such peak, and it works as follows. First, the horizontal changes are computed:\n",
    "\n",
    "$$G_{x} = \\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix} * I$$.\n",
    "\n",
    "Then, the vertical changes are computed:\n",
    "\n",
    "$$G_{y} = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix} * I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Finally, an approximation of the gradient in that point by combining both results above is computed:\n",
    "\n",
    "$$G = \\sqrt{ G_{x}^{2} + G_{y}^{2} }$$\n",
    "\n",
    "or in its easier form\n",
    "\n",
    "$$G = |G_{x}| + |G_{y}| $$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In OpenCV, these steps are computed with the `Sobel` function, that takes the following arguments:\n",
    "\n",
    "- src\tinput image.\n",
    "- dst\toutput image of the same size and the same number of channels as src .\n",
    "- ddepth\toutput image depth, see combinations in documentation; in the case of 8-bit input images it will result in truncated derivatives.\n",
    "- dx\torder of the derivative x.\n",
    "- dy\torder of the derivative y.\n",
    "- ksize\tsize of the extended Sobel kernel; it must be 1, 3, 5, or 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's see how to use Sobel. First, we convert image to grayscale (we do not need color for detecting edges):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Next, we compute the *x* and *y* derivatives with Sobel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad_x = cv.Sobel(gray, ddepth, 1, 0)\n",
    "grad_y = cv.Sobel(gray, ddepth, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Notice that we cannot assume that the derivative is positive! Hence, we have to scale and convert our gradients to a single channel image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "abs_grad_x = cv.convertScaleAbs(grad_x)\n",
    "abs_grad_y = cv.convertScaleAbs(grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "To obtain the final image containing the edges, we can add together the two gradients images. To do so, we can use the function `addWeighted`. Such function allows to add together two images and to specify their weight, i.e., to give more importance to an image with respect to the other. Since we want to give to both gradients the same importance, we provide $0.5$ as weight for both images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grad = cv.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For the Laplacian operator, we just need to call the corresponding OpenCV function and to convert the result into a `uint8` image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply Laplace function\n",
    "dst = cv.Laplacian(src_gray, ddepth, ksize=kernel_size)\n",
    "\n",
    "# converting back to uint8\n",
    "abs_dst = cv.convertScaleAbs(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Image Arithmetic\n",
    "\n",
    "Now, we will learn about some common arithmetic operations that can be performed on images, such as bitwise operations, addition, and subtraction, among others. Notice that, OpenCV functions works with the saturation arithmetic. Saturation arithmetic is a type of arithmetic operation where the operations are limited to a fixed range by restricting the maximum and minimum values that the operation can take.\n",
    "\n",
    "In OpenCV, the values are clipped to ensure that they will never fall outside the range $[0,255]$, while in Numpy the values will overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "x = np.uint8([250])\n",
    "y = np.uint8([50])\n",
    "\n",
    "result_opencv = cv2.add(x, y)\n",
    "print(f\"cv2.add({x}, {y}) = {result_opencv}\")\n",
    "\n",
    "result_numpy = x + y\n",
    "print(f\"{x} + {y} = {result_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition and Subtraction\n",
    "\n",
    "Image addition and subtraction can be performed with the `cv2.add()` and `cv2.subtract()` functions, respectively. These functions can be also used for adding/subtracting a scalar to/from an image.\n",
    "\n",
    "Suppose that we want to add the value *50* to all the pixels of an image. Firstly, we create the image to be added to the one that we wants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = np.full(img.shape,50,dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now, we can add the image containing all *50*s to our image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "added_image = cv2.add(img, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we want to use a scalar notation, you can use the following instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = np.full((1,3),50,dtype=\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For performing the subtraction, the principles are the same but you just need to call the `cv2.subtract` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Blending\n",
    "\n",
    "We have already seen this operation with the Sobel operator. Blending is also image addition, but different weights are given to the images, giving an impression of transparency. In order to do this, the `cv2.addWeighted()` function will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Bitwise Operations\n",
    "\n",
    "There are some operations that can be performed at bit level using bitwise operators, which can be used to manipulate the values for comparison and calculations. These bitwise operations are simple, and are quick to calculate. The available bitwise operations are:\n",
    "\n",
    "- AND: `cv2.bitwise_and(img_1, img_2)`\n",
    "- OR: `cv2.bitwise_or(img_1, img_2)`\n",
    "- XOR: `cv2.bitwise_xor(img_1, img_2)`\n",
    "- NOT: `cv2.bitwise_not(img_1, img_2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/6.jpg\" height=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Histograms\n",
    "\n",
    "An image histogram is a type of histogram that reflects the tonal distribution of the image, plotting the number of pixels for each tonal value. The number of pixels for each tonal value is also called frequency. Therefore, a histogram for a grayscale image with intensity\n",
    "values in the range $[0, K-1]$ will contain exactly K entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src =\"imgs/7.jpg\" height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Before moving on with histograms, we need to introduce 2 terms:\n",
    "\n",
    "- *bins*: Each frequency shown within the histogram is called bin. In a grayscale image, if we consider each value between $[0,255]$, we will have 256 bins. However, the number of bins can be chosen as desired. In OpenCV, the number of bins is referred as `histSize`\n",
    "- *range*: It is the range of values that we want to consider. If we want all the values of a channel, this is set to $[0,255]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "OpenCV provides the `cv2.calcHist()` function in order to calculate the histogram of one or more arrays. Therefore, this function can be applied to single-channel images (for example, grayscale images) and to multi-channel images (for example, BGR images):\n",
    "\n",
    "cv2.calcHist(images, channels, mask, histSize, ranges[, hist[,accumulate]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "where:\n",
    "\n",
    "- images: It represents the source image of type uint8 or float32 provided as a list\n",
    "- channels: It represents the index of the channel for which we calculate the histogram provided as a list\n",
    "- mask: It represents a mask image to calculate the histogram of a specific region of the image defined by the mask. If this parameter is equal to None, the histogram will be calculated with no mask and the full image will be used\n",
    "- histSize: It represents the number of bins provided as a list\n",
    "- ranges: It represents the range of intensity values we want to measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's start by computing the histogram of a grayscale image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('lenna.png')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For plotting the histogram, matplotlib will be used. You can plot histogram with 2 functions: `hist` and the standard `plot` function. Let's see how to plot the histogram for a color image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv.imread('data/wave.png')\n",
    "color = ('b','g','r')\n",
    "for i,col in enumerate(color):\n",
    "    histr = cv.calcHist([img],[i],None,[256],[0,256])\n",
    "    plt.plot(histr,color = col)\n",
    "    plt.xlim([0,256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Instead, with the `hist` function we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv.imread('data/lenna.png')\n",
    "color = ('b','g','r')\n",
    "for i,col in enumerate(color):\n",
    "    channel = img[:,:,i]\n",
    "    plt.hist(channel.ravel(),256,[0,256], color=col)\n",
    "    plt.xlim([0,256])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now that we have the histogram of an image, it is possible to use it for fixing the colours inside the image (if needed!). This process is called *histogram equalization*, and it is used to normalize the brightness and to increase the contrast of the image. Let's see how it works with grayscale images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('lenna.png',cv2.IMREAD_GRAYSCALE)\n",
    "gray_image_eq = cv2.equalizeHist(gray_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As you may see, the function `equalizeHist` automatically computes the histogram. Let's see how it work with colo images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"data/lena.png\")\n",
    "\n",
    "channels = cv2.split(img)\n",
    "eq_channels = []\n",
    "\n",
    "for ch in channels:\n",
    "    eq_channels.append(cv2.equalizeHist(ch))\n",
    "    \n",
    "equalized = cv2.merge(eq_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Despite the new image may look good, its colours drastically changed due to the additive nature of the RGB color space. Hence, the best equalization approach is to change the color space of our image to one having the luminance/intensity channel, such as HSV:\n",
    "\n",
    "<img src=\"imgs/8.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In HSV image, we still have 3 channels, but they are:\n",
    "\n",
    "- Hue: the color itself\n",
    "- Saturation: the \"intensity\"of the color\n",
    "- Value: the \"brightness\" of the color\n",
    "\n",
    "Let's see how to equalize the histogram of our image by using the HSV color space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"data/lena.png\")\n",
    "\n",
    "img_original = img.copy()\n",
    "hsv_img = cv2.cvtColor(img_original,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "h,s,v = cv2.split(hsv_img)\n",
    "\n",
    "eq_v = cv2.equalizeHist(v)\n",
    "\n",
    "equalized = cv2.merge([h,s,eq_v])\n",
    "equalized = cv2.cvtColor(equalized,cv2.COLOR_HSV2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Sometimes, the standard equalization process do not provides good results. Let's have a look at the following images:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"imgs/9.jpg\" width=400> </td>\n",
    "        <td> <img src=\"imgs/10.jpg\" width=400> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> Original Image </td>\n",
    "        <td> After Equalization </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It is true that the background contrast has improved after histogram equalization. But compare the face of statue in both images. We lost most of the information there due to over-brightness. It is because standard histogram is a global technique, i.e. considers all the pixels of the image.\n",
    "\n",
    "To solve this problem, adaptive histogram equalization is used. In this, image is divided into small blocks called \"tiles\" (tileSize is 8x8 by default in OpenCV). Then each of these blocks are histogram equalized as usual. So in a small area, histogram would confine to a small region (unless there is noise). If noise is there, it will be amplified. To avoid this, contrast limiting is applied. If any histogram bin is above the specified contrast limit (by default 40 in OpenCV), those pixels are clipped and distributed uniformly to other bins before applying histogram equalization. After equalization, to remove artifacts in tile borders, bilinear interpolation is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Such method is called CLAHE (Contrast Limited Adaptive Histogram Equalization), and can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv.imread('data/tsukuba_l.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# create a CLAHE object (Arguments are optional).\n",
    "clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "equalized = clahe.apply(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As it is possible to see, now the image is correctly equalized:\n",
    "\n",
    "<img src=\"imgs/11.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Features Extraction\n",
    "\n",
    "Now we are at the last topic regarding image processing: the image descriptors. As the name suggests, such descriptors are (usually) vectors containing values representing the salient features of the image. Such descriptors can be used for several tasks, such as image-based searches, panorama stitching, object detection/classification, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Usually, finding the descriptors is a twofold process:\n",
    "\n",
    "- Find the features positions\n",
    "- Store the characteristics of such special points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "How can we find the position of these point inside our images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "It depends on the type of point that you want to find!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "There are algorithms that allow you to find corners in images, such as **Harris** and **FAST**, while others allow to find blobs, such as **SIFT, SURF, BRIEF, ORB,A-KAZE**.\n",
    "\n",
    "With these things being said, let's see how these keypoints works. In our example, we will use Harris to extract corner from images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# load image\n",
    "img = cv2.imread(r\"data/lena.png\")\n",
    "\n",
    "# convert to gray\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# detect corners\n",
    "dst = cv2.cornerHarris(gray,2,23,0.04)\n",
    "\n",
    "# Threshold for getting the optimal values, must be tuning depending on the image\n",
    "img[dst > 0.01 * dst.max()] = [0,0,255]\n",
    "cv2.imshow(\"test\",dst)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The above code will extract the corners in our image. Let's talk about the Harris function parameters:\n",
    "\n",
    "\n",
    "- img, Input image. It should be grayscale and float32 type\n",
    "- blockSize, It is the size of neighbourhood considered for corner detection\n",
    "- ksize, Aperture parameter of the Sobel derivative used.\n",
    "- k, Harris detector free parameter in the equation\n",
    "\n",
    "As a result, we obtain a matrix containing the score for each detected corner. The higher the score, the higher is the probability that we have found a corner. \n",
    "\n",
    "Now let's see what happens if we change the scale of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# resize the image\n",
    "resized = cv2.resize(img,None,fx=0.5,fy=0.5)\n",
    "\n",
    "# convert to gray\n",
    "gray_resized = cv2.cvtColor(resized,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect corners\n",
    "dst = cv2.cornerHarris(gray_resized,2,23,0.04)\n",
    "\n",
    "# highlights the corners\n",
    "resized[dst > 0.01 * dst.max()] = [0,0,255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "As it is possible to see, the result is different! This aspect introduces an issue: this feature extractor is not invariant to scale!\n",
    "\n",
    "This is a severe problem in applications in which it is not guaranteed the same scale in each image, i.e. drones applications, panoramas, etc.\n",
    "\n",
    "For this reason, we need an algorithm that allows us to extract features invariant to the scale property. One first algorithm is **Scale-Invariant Feature Transform**, or simply SIFT. The following code shows how to use it in OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(r\"data/lena.png\")\n",
    "\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "keypoints, descriptors = sift.detectAndCompute(img,None)\n",
    "cv2.drawKeypoints(img, keypoints, img, (51, 163, 236),cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow(\"test\",img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The first thing that we need to do is to declare our feature extractor, in this case, SIFT. Then, with the `detectAndCompute` function two things happen:\n",
    "\n",
    "- We first find the keypoint, and then\n",
    "- We compute the descriptor for each keypoint found\n",
    "\n",
    "At this point, what is the difference between a keypoint and a descriptor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Depending on the algorithm used to extract keypoint, you will know some general characteristics of the extracted keypoints (e.g. they are centered around blobs, edges, etc.), but you will not know how different or similar one keypoint is to the other. That is the purpose of the descriptors: they allow to compare keypoints. In fact, descriptors are values that should be:\n",
    "\n",
    "- independent from keypoint position\n",
    "- robust against image transformations\n",
    "- scale independent\n",
    "- illumination independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Each computed keypoint is an instance of the class `cv2.Keypoint`, and contains the following informations:\n",
    "\n",
    "- The pt (point) property contains the x and y coordinates of the keypoint in the image.\n",
    "- The size property indicates the diameter of the feature.\n",
    "- The angle property indicates the orientation of the feature, as shown by the radial lines in the preceding processed image.\n",
    "- The response property indicates the strength of the keypoint. Some features are classified by SIFT as stronger than others, and response is the property you would check to evaluate the strength of a feature.\n",
    "- The octave property indicates the layer in the image pyramid where the feature was found. \n",
    "- The class_id property can be used to assign a custom identifier to a keypoint or a group of keypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Despite SIFT works well in most of the situations, it has a drawback: it is slow. To overcome this problem, SURF algorithm was proposed. The difference between SIFT and SURF is that on the former the features are extracted by using the Difference of Gaussians, while the latter uses the Fast Hessian algorithm. In addition, SURF descriptor has a size of 64 elements, while SIFT descriptor size is 125. In OpenCV, the extraction of features has been standardized, hence we can use the SURF detector in the same way of SIFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the image\n",
    "img = cv2.imread('data/lena.png')\n",
    "\n",
    "# create the instance of the extractor and set a threshold\n",
    "surf = cv2.xfeatures2d.SURF_create(8000)\n",
    "\n",
    "# feature extraction\n",
    "keypoints, descriptor = surf.detectAndCompute(gray, None)\n",
    "\n",
    "cv2.drawKeypoints(img, keypoints, img, (51, 163, 236),\n",
    "cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('surf_keypoints', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Other very used feature extractors are ORB and A-KAZE, which are faster and have a binary descriptor, but they are less reliable. Now that we know how to extract features from images, we can search for the same features among different images, namely, we can match them. In the following examples, we will use the Brute-Force Matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The Brute-Force Matcher (BFM) is the most simplest algorithm. For each keypoint descriptor in the first set, the matcher makes comparisons to every keypoint descriptor in the second set. Each comparison produces a distance value and the best match can be chosen on the basis of least distance. Let's see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the images.\n",
    "img0 = cv2.imread('data/1.png')\n",
    "img1 = cv2.imread('data/2.png')\n",
    "\n",
    "# Perform ORB feature detection and description.\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "kp0, des0 = orb.detectAndCompute(img0, None)\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "\n",
    "# Perform brute-force matching.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des0, des1)\n",
    "\n",
    "# Sort the matches by distance.\n",
    "matches = sorted(matches, key=lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw the best 25 matches.\n",
    "img_matches = cv2.drawMatches(img0, kp0, img1, kp1, matches[:25], img1,flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "# Show the matches.\n",
    "cv2.imshow(img_matches)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "To speedup the matching, the BFM allows to match with only the *k* nearest keypoints. This is possible in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform brute-force KNN matching.\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "pairs_of_matches = bf.knnMatch(des0, des1, k=2) # the two nearest keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "`knnMatch` returns a list of lists; each inner list contains at least one match and no more than k matches, sorted from best (least distance) to worst. The following line of code sorts the outer list based on the distance score of the best matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort the pairs of matches by distance.\n",
    "pairs_of_matches = sorted(pairs_of_matches, key=lambda x:x[0].distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the original paper proposing the features as an object detection method, it is stated that **\"The probability that a match is correct can be determined by taking the ratio of the distance from the closest neighbor to the distance of the second closest.\"** Hence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the ratio test.\n",
    "best_matches = [x[0] for x in pairs_of_matches if len(x) > 1 and x[0].distance < 0.8 * x[1].distance]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
